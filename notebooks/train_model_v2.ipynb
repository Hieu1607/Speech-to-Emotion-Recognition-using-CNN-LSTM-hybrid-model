{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a906dd",
   "metadata": {},
   "source": [
    "# This notebook expressed the training progress for CNN-LSTM version 2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447c2e5",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98467e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6e088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10412941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2367</th>\n",
       "      <th>2368</th>\n",
       "      <th>2369</th>\n",
       "      <th>2370</th>\n",
       "      <th>2371</th>\n",
       "      <th>2372</th>\n",
       "      <th>2373</th>\n",
       "      <th>2374</th>\n",
       "      <th>2375</th>\n",
       "      <th>Emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.264648</td>\n",
       "      <td>0.372559</td>\n",
       "      <td>0.486328</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.681152</td>\n",
       "      <td>0.720703</td>\n",
       "      <td>0.651855</td>\n",
       "      <td>0.562988</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766315</td>\n",
       "      <td>0.755441</td>\n",
       "      <td>0.743671</td>\n",
       "      <td>0.731043</td>\n",
       "      <td>0.717561</td>\n",
       "      <td>0.703260</td>\n",
       "      <td>0.688124</td>\n",
       "      <td>0.672208</td>\n",
       "      <td>0.655518</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.231934</td>\n",
       "      <td>0.357422</td>\n",
       "      <td>0.482910</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.480469</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.491699</td>\n",
       "      <td>0.490723</td>\n",
       "      <td>0.500977</td>\n",
       "      <td>0.503418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.594981</td>\n",
       "      <td>-1.356783</td>\n",
       "      <td>3.955091</td>\n",
       "      <td>1.570934</td>\n",
       "      <td>1.334768</td>\n",
       "      <td>0.901523</td>\n",
       "      <td>-0.066584</td>\n",
       "      <td>-6.374361</td>\n",
       "      <td>-7.761413</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.256348</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>0.471191</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>0.373535</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>0.408691</td>\n",
       "      <td>0.466797</td>\n",
       "      <td>0.522461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.384766</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.764160</td>\n",
       "      <td>0.778809</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.752930</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.799316</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>...</td>\n",
       "      <td>6.918442</td>\n",
       "      <td>6.534257</td>\n",
       "      <td>6.109815</td>\n",
       "      <td>5.613272</td>\n",
       "      <td>5.033775</td>\n",
       "      <td>4.383093</td>\n",
       "      <td>3.692577</td>\n",
       "      <td>3.005888</td>\n",
       "      <td>2.370324</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.389648</td>\n",
       "      <td>0.564941</td>\n",
       "      <td>0.642578</td>\n",
       "      <td>0.665527</td>\n",
       "      <td>0.684570</td>\n",
       "      <td>0.676270</td>\n",
       "      <td>0.666992</td>\n",
       "      <td>0.648926</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48643</th>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.035645</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>...</td>\n",
       "      <td>2.149720</td>\n",
       "      <td>-1.269249</td>\n",
       "      <td>8.457794</td>\n",
       "      <td>-0.209345</td>\n",
       "      <td>3.439721</td>\n",
       "      <td>10.229767</td>\n",
       "      <td>14.676046</td>\n",
       "      <td>4.467748</td>\n",
       "      <td>4.202962</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48644</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>0.025879</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.478354</td>\n",
       "      <td>-5.067346</td>\n",
       "      <td>-4.775791</td>\n",
       "      <td>0.778204</td>\n",
       "      <td>5.260514</td>\n",
       "      <td>-0.345362</td>\n",
       "      <td>-1.215722</td>\n",
       "      <td>8.969276</td>\n",
       "      <td>7.940011</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48645</th>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>0.163574</td>\n",
       "      <td>0.132324</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.975361</td>\n",
       "      <td>-4.235834</td>\n",
       "      <td>-2.939499</td>\n",
       "      <td>-1.558753</td>\n",
       "      <td>0.182943</td>\n",
       "      <td>2.841793</td>\n",
       "      <td>4.177210</td>\n",
       "      <td>4.016248</td>\n",
       "      <td>0.110962</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48646</th>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.944071</td>\n",
       "      <td>-13.947256</td>\n",
       "      <td>-3.418745</td>\n",
       "      <td>6.305834</td>\n",
       "      <td>9.191437</td>\n",
       "      <td>-7.628603</td>\n",
       "      <td>1.824499</td>\n",
       "      <td>13.876075</td>\n",
       "      <td>-1.117508</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48647</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>0.030762</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.942566</td>\n",
       "      <td>0.478401</td>\n",
       "      <td>11.062085</td>\n",
       "      <td>11.928731</td>\n",
       "      <td>16.490311</td>\n",
       "      <td>10.112006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.381580</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48648 rows × 2377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.173828  0.264648  0.372559  0.486328  0.625488  0.681152  0.720703   \n",
       "1      0.231934  0.357422  0.482910  0.472656  0.480469  0.484375  0.491699   \n",
       "2      0.256348  0.354492  0.471191  0.414062  0.378418  0.373535  0.354492   \n",
       "3      0.384766  0.576172  0.764160  0.778809  0.742188  0.752930  0.787109   \n",
       "4      0.247070  0.389648  0.564941  0.642578  0.665527  0.684570  0.676270   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "48643  0.025391  0.035645  0.043457  0.076660  0.116211  0.119141  0.121094   \n",
       "48644  0.000000  0.000000  0.006348  0.016113  0.025879  0.034180  0.036621   \n",
       "48645  0.078125  0.125977  0.163574  0.132324  0.104980  0.066406  0.065918   \n",
       "48646  0.004883  0.015625  0.026367  0.036621  0.042969  0.041504  0.043457   \n",
       "48647  0.019531  0.026367  0.036133  0.032715  0.032227  0.031250  0.029297   \n",
       "\n",
       "              7         8         9  ...       2367       2368       2369  \\\n",
       "0      0.651855  0.562988  0.559082  ...   0.766315   0.755441   0.743671   \n",
       "1      0.490723  0.500977  0.503418  ...   1.594981  -1.356783   3.955091   \n",
       "2      0.408691  0.466797  0.522461  ...   0.006713   0.006601   0.006483   \n",
       "3      0.799316  0.700195  0.590332  ...   6.918442   6.534257   6.109815   \n",
       "4      0.666992  0.648926  0.595703  ...   0.000000   0.000000   0.000000   \n",
       "...         ...       ...       ...  ...        ...        ...        ...   \n",
       "48643  0.081543  0.038574  0.032715  ...   2.149720  -1.269249   8.457794   \n",
       "48644  0.033691  0.032715  0.032227  ...  -4.478354  -5.067346  -4.775791   \n",
       "48645  0.074707  0.076660  0.081055  ...  -5.975361  -4.235834  -2.939499   \n",
       "48646  0.043457  0.041504  0.042480  ... -17.944071 -13.947256  -3.418745   \n",
       "48647  0.028320  0.028809  0.030762  ... -13.942566   0.478401  11.062085   \n",
       "\n",
       "            2370       2371       2372       2373       2374      2375  \\\n",
       "0       0.731043   0.717561   0.703260   0.688124   0.672208  0.655518   \n",
       "1       1.570934   1.334768   0.901523  -0.066584  -6.374361 -7.761413   \n",
       "2       0.006356   0.006218   0.006066   0.005919   0.005759  0.005588   \n",
       "3       5.613272   5.033775   4.383093   3.692577   3.005888  2.370324   \n",
       "4       0.000000   0.000000   0.000000   0.000000   0.000000  0.000000   \n",
       "...          ...        ...        ...        ...        ...       ...   \n",
       "48643  -0.209345   3.439721  10.229767  14.676046   4.467748  4.202962   \n",
       "48644   0.778204   5.260514  -0.345362  -1.215722   8.969276  7.940011   \n",
       "48645  -1.558753   0.182943   2.841793   4.177210   4.016248  0.110962   \n",
       "48646   6.305834   9.191437  -7.628603   1.824499  13.876075 -1.117508   \n",
       "48647  11.928731  16.490311  10.112006   0.000000   0.000000 -1.381580   \n",
       "\n",
       "       Emotions  \n",
       "0       neutral  \n",
       "1       neutral  \n",
       "2       neutral  \n",
       "3       neutral  \n",
       "4       neutral  \n",
       "...         ...  \n",
       "48643  surprise  \n",
       "48644  surprise  \n",
       "48645  surprise  \n",
       "48646  surprise  \n",
       "48647  surprise  \n",
       "\n",
       "[48648 rows x 2377 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add project root to Python path for imports\n",
    "import os\n",
    "project_root = os.path.abspath('..')  # Go up one level from notebooks to project root\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Load processed data\n",
    "data_path = os.path.join(project_root, 'data', 'processed', 'emotion.csv')\n",
    "data = pd.read_csv(data_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75199f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           False\n",
      "1           False\n",
      "2           False\n",
      "3           False\n",
      "4           False\n",
      "            ...  \n",
      "2372         True\n",
      "2373         True\n",
      "2374         True\n",
      "2375         True\n",
      "Emotions    False\n",
      "Length: 2377, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Check if data has any NaN values\n",
    "print(data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b778cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           False\n",
      "1           False\n",
      "2           False\n",
      "3           False\n",
      "4           False\n",
      "            ...  \n",
      "2372        False\n",
      "2373        False\n",
      "2374        False\n",
      "2375        False\n",
      "Emotions    False\n",
      "Length: 2377, dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48648, 2377)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In preprocessing data, there are so many problems which can create NaN values, so in this situation, we just replace them with 0 to make it simple.\n",
    "Emotions= data.fillna(0)\n",
    "print(Emotions.isna().any())\n",
    "Emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83211cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "           ..\n",
       "2372        0\n",
       "2373        0\n",
       "2374        0\n",
       "2375        0\n",
       "Emotions    0\n",
       "Length: 2377, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Emotions.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162825d",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6d9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking all rows and all cols without last col for X which include features\n",
    "# Taking last col for Y, which include the emotions\n",
    "\n",
    "X,Y = Emotions.iloc[:,:-1].values, Emotions['Emotions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f142003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8263c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (48648, 2376)\n",
      "Y shape:  (48648, 7)\n"
     ]
    }
   ],
   "source": [
    "print('X shape: ',X.shape)\n",
    "print('Y shape: ',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492f1c6",
   "metadata": {},
   "source": [
    "## Train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e3438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define const values\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM_RATE = 0.9\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ffdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "full_dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51f25cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 38918\n",
      "Validation dataset size: 4864\n",
      "Test dataset size: 4866\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "sizes = [train_size, val_size, test_size]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset=full_dataset, lengths=sizes, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(val_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88d7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# DataLoader for training dataset (Training): BẬT shuffle\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# DataLoader for validation dataset (Validation): TẮT shuffle\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# DataLoader for test dataset (Test): TẮT shuffle\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a2ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "from src.model.CNN_LSTM_model import create_model\n",
    "\n",
    "model = create_model(input_size=X.shape[-1], n_emotions=Y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4bed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    params=model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    momentum=MOMENTUM_RATE,\n",
    "    nesterov=True # Enable Nesterov momentum\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ddfb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reduce learning rate on plateau scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',        \n",
    "    factor=0.5,        \n",
    "    patience=3,\n",
    "    min_lr=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f10152ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]:\n",
      "  Train Loss: 1.2754, Train Acc: 0.5017\n",
      "  Val Loss: 1.4430, Val Acc: 0.4305\n",
      "  New best validation accuracy: 0.4305\n",
      "Epoch [2/50]:\n",
      "  Train Loss: 1.1946, Train Acc: 0.5325\n",
      "  Val Loss: 1.3103, Val Acc: 0.4928\n",
      "  New best validation accuracy: 0.4928\n",
      "Epoch [2/50]:\n",
      "  Train Loss: 1.1946, Train Acc: 0.5325\n",
      "  Val Loss: 1.3103, Val Acc: 0.4928\n",
      "  New best validation accuracy: 0.4928\n",
      "Epoch [3/50]:\n",
      "  Train Loss: 1.1337, Train Acc: 0.5534\n",
      "  Val Loss: 1.2034, Val Acc: 0.5403\n",
      "  New best validation accuracy: 0.5403\n",
      "Epoch [3/50]:\n",
      "  Train Loss: 1.1337, Train Acc: 0.5534\n",
      "  Val Loss: 1.2034, Val Acc: 0.5403\n",
      "  New best validation accuracy: 0.5403\n",
      "Epoch [4/50]:\n",
      "  Train Loss: 1.0823, Train Acc: 0.5793\n",
      "  Val Loss: 1.2553, Val Acc: 0.5148\n",
      "Epoch [4/50]:\n",
      "  Train Loss: 1.0823, Train Acc: 0.5793\n",
      "  Val Loss: 1.2553, Val Acc: 0.5148\n",
      "Epoch [5/50]:\n",
      "  Train Loss: 1.0356, Train Acc: 0.5978\n",
      "  Val Loss: 1.1518, Val Acc: 0.5432\n",
      "  New best validation accuracy: 0.5432\n",
      "Epoch [5/50]:\n",
      "  Train Loss: 1.0356, Train Acc: 0.5978\n",
      "  Val Loss: 1.1518, Val Acc: 0.5432\n",
      "  New best validation accuracy: 0.5432\n",
      "Epoch [6/50]:\n",
      "  Train Loss: 0.9968, Train Acc: 0.6135\n",
      "  Val Loss: 1.1114, Val Acc: 0.5711\n",
      "  New best validation accuracy: 0.5711\n",
      "Epoch [6/50]:\n",
      "  Train Loss: 0.9968, Train Acc: 0.6135\n",
      "  Val Loss: 1.1114, Val Acc: 0.5711\n",
      "  New best validation accuracy: 0.5711\n",
      "Epoch [7/50]:\n",
      "  Train Loss: 0.9564, Train Acc: 0.6301\n",
      "  Val Loss: 1.2673, Val Acc: 0.5298\n",
      "Epoch [7/50]:\n",
      "  Train Loss: 0.9564, Train Acc: 0.6301\n",
      "  Val Loss: 1.2673, Val Acc: 0.5298\n",
      "Epoch [8/50]:\n",
      "  Train Loss: 0.9151, Train Acc: 0.6466\n",
      "  Val Loss: 1.1432, Val Acc: 0.5559\n",
      "Epoch [8/50]:\n",
      "  Train Loss: 0.9151, Train Acc: 0.6466\n",
      "  Val Loss: 1.1432, Val Acc: 0.5559\n",
      "Epoch [9/50]:\n",
      "  Train Loss: 0.8773, Train Acc: 0.6654\n",
      "  Val Loss: 1.1330, Val Acc: 0.5715\n",
      "  New best validation accuracy: 0.5715\n",
      "Epoch [9/50]:\n",
      "  Train Loss: 0.8773, Train Acc: 0.6654\n",
      "  Val Loss: 1.1330, Val Acc: 0.5715\n",
      "  New best validation accuracy: 0.5715\n",
      "Epoch [10/50]:\n",
      "  Train Loss: 0.8414, Train Acc: 0.6789\n",
      "  Val Loss: 1.0831, Val Acc: 0.5925\n",
      "  New best validation accuracy: 0.5925\n",
      "Epoch [10/50]:\n",
      "  Train Loss: 0.8414, Train Acc: 0.6789\n",
      "  Val Loss: 1.0831, Val Acc: 0.5925\n",
      "  New best validation accuracy: 0.5925\n",
      "Epoch [11/50]:\n",
      "  Train Loss: 0.8010, Train Acc: 0.6932\n",
      "  Val Loss: 1.3039, Val Acc: 0.5368\n",
      "Epoch [11/50]:\n",
      "  Train Loss: 0.8010, Train Acc: 0.6932\n",
      "  Val Loss: 1.3039, Val Acc: 0.5368\n",
      "Epoch [12/50]:\n",
      "  Train Loss: 0.7682, Train Acc: 0.7068\n",
      "  Val Loss: 1.1222, Val Acc: 0.5765\n",
      "Epoch [12/50]:\n",
      "  Train Loss: 0.7682, Train Acc: 0.7068\n",
      "  Val Loss: 1.1222, Val Acc: 0.5765\n",
      "Epoch [13/50]:\n",
      "  Train Loss: 0.7302, Train Acc: 0.7247\n",
      "  Val Loss: 1.0987, Val Acc: 0.5833\n",
      "Epoch [13/50]:\n",
      "  Train Loss: 0.7302, Train Acc: 0.7247\n",
      "  Val Loss: 1.0987, Val Acc: 0.5833\n",
      "Epoch [14/50]:\n",
      "  Train Loss: 0.6988, Train Acc: 0.7341\n",
      "  Val Loss: 1.2416, Val Acc: 0.5781\n",
      "Epoch [14/50]:\n",
      "  Train Loss: 0.6988, Train Acc: 0.7341\n",
      "  Val Loss: 1.2416, Val Acc: 0.5781\n",
      "Epoch [15/50]:\n",
      "  Train Loss: 0.5751, Train Acc: 0.7849\n",
      "  Val Loss: 1.0613, Val Acc: 0.6258\n",
      "  New best validation accuracy: 0.6258\n",
      "Epoch [15/50]:\n",
      "  Train Loss: 0.5751, Train Acc: 0.7849\n",
      "  Val Loss: 1.0613, Val Acc: 0.6258\n",
      "  New best validation accuracy: 0.6258\n",
      "Epoch [16/50]:\n",
      "  Train Loss: 0.5283, Train Acc: 0.8044\n",
      "  Val Loss: 1.0241, Val Acc: 0.6375\n",
      "  New best validation accuracy: 0.6375\n",
      "Epoch [16/50]:\n",
      "  Train Loss: 0.5283, Train Acc: 0.8044\n",
      "  Val Loss: 1.0241, Val Acc: 0.6375\n",
      "  New best validation accuracy: 0.6375\n",
      "Epoch [17/50]:\n",
      "  Train Loss: 0.4990, Train Acc: 0.8172\n",
      "  Val Loss: 1.0800, Val Acc: 0.6386\n",
      "  New best validation accuracy: 0.6386\n",
      "Epoch [17/50]:\n",
      "  Train Loss: 0.4990, Train Acc: 0.8172\n",
      "  Val Loss: 1.0800, Val Acc: 0.6386\n",
      "  New best validation accuracy: 0.6386\n",
      "Epoch [18/50]:\n",
      "  Train Loss: 0.4852, Train Acc: 0.8212\n",
      "  Val Loss: 1.1387, Val Acc: 0.6262\n",
      "Epoch [18/50]:\n",
      "  Train Loss: 0.4852, Train Acc: 0.8212\n",
      "  Val Loss: 1.1387, Val Acc: 0.6262\n",
      "Epoch [19/50]:\n",
      "  Train Loss: 0.4549, Train Acc: 0.8333\n",
      "  Val Loss: 1.0689, Val Acc: 0.6396\n",
      "  New best validation accuracy: 0.6396\n",
      "Epoch [19/50]:\n",
      "  Train Loss: 0.4549, Train Acc: 0.8333\n",
      "  Val Loss: 1.0689, Val Acc: 0.6396\n",
      "  New best validation accuracy: 0.6396\n",
      "Epoch [20/50]:\n",
      "  Train Loss: 0.4282, Train Acc: 0.8443\n",
      "  Val Loss: 1.6424, Val Acc: 0.5648\n",
      "Epoch [20/50]:\n",
      "  Train Loss: 0.4282, Train Acc: 0.8443\n",
      "  Val Loss: 1.6424, Val Acc: 0.5648\n",
      "Epoch [21/50]:\n",
      "  Train Loss: 0.4215, Train Acc: 0.8469\n",
      "  Val Loss: 1.1651, Val Acc: 0.6297\n",
      "Epoch [21/50]:\n",
      "  Train Loss: 0.4215, Train Acc: 0.8469\n",
      "  Val Loss: 1.1651, Val Acc: 0.6297\n",
      "Epoch [22/50]:\n",
      "  Train Loss: 0.3962, Train Acc: 0.8555\n",
      "  Val Loss: 1.1528, Val Acc: 0.6447\n",
      "  New best validation accuracy: 0.6447\n",
      "Epoch [22/50]:\n",
      "  Train Loss: 0.3962, Train Acc: 0.8555\n",
      "  Val Loss: 1.1528, Val Acc: 0.6447\n",
      "  New best validation accuracy: 0.6447\n",
      "Epoch [23/50]:\n",
      "  Train Loss: 0.3735, Train Acc: 0.8645\n",
      "  Val Loss: 1.1920, Val Acc: 0.6439\n",
      "Epoch [23/50]:\n",
      "  Train Loss: 0.3735, Train Acc: 0.8645\n",
      "  Val Loss: 1.1920, Val Acc: 0.6439\n",
      "Epoch [24/50]:\n",
      "  Train Loss: 0.3528, Train Acc: 0.8730\n",
      "  Val Loss: 1.1852, Val Acc: 0.6351\n",
      "Epoch [24/50]:\n",
      "  Train Loss: 0.3528, Train Acc: 0.8730\n",
      "  Val Loss: 1.1852, Val Acc: 0.6351\n",
      "Epoch [25/50]:\n",
      "  Train Loss: 0.3319, Train Acc: 0.8804\n",
      "  Val Loss: 1.2017, Val Acc: 0.6429\n",
      "Epoch [25/50]:\n",
      "  Train Loss: 0.3319, Train Acc: 0.8804\n",
      "  Val Loss: 1.2017, Val Acc: 0.6429\n",
      "Epoch [26/50]:\n",
      "  Train Loss: 0.3170, Train Acc: 0.8869\n",
      "  Val Loss: 1.2695, Val Acc: 0.6332\n",
      "Epoch [26/50]:\n",
      "  Train Loss: 0.3170, Train Acc: 0.8869\n",
      "  Val Loss: 1.2695, Val Acc: 0.6332\n",
      "Epoch [27/50]:\n",
      "  Train Loss: 0.2228, Train Acc: 0.9281\n",
      "  Val Loss: 1.1664, Val Acc: 0.6577\n",
      "  New best validation accuracy: 0.6577\n",
      "Epoch [27/50]:\n",
      "  Train Loss: 0.2228, Train Acc: 0.9281\n",
      "  Val Loss: 1.1664, Val Acc: 0.6577\n",
      "  New best validation accuracy: 0.6577\n",
      "Epoch [28/50]:\n",
      "  Train Loss: 0.1900, Train Acc: 0.9410\n",
      "  Val Loss: 1.2328, Val Acc: 0.6560\n",
      "Epoch [28/50]:\n",
      "  Train Loss: 0.1900, Train Acc: 0.9410\n",
      "  Val Loss: 1.2328, Val Acc: 0.6560\n",
      "Epoch [29/50]:\n",
      "  Train Loss: 0.1844, Train Acc: 0.9408\n",
      "  Val Loss: 1.2500, Val Acc: 0.6595\n",
      "  New best validation accuracy: 0.6595\n",
      "Epoch [29/50]:\n",
      "  Train Loss: 0.1844, Train Acc: 0.9408\n",
      "  Val Loss: 1.2500, Val Acc: 0.6595\n",
      "  New best validation accuracy: 0.6595\n",
      "Epoch [30/50]:\n",
      "  Train Loss: 0.1690, Train Acc: 0.9481\n",
      "  Val Loss: 1.2915, Val Acc: 0.6550\n",
      "Epoch [30/50]:\n",
      "  Train Loss: 0.1690, Train Acc: 0.9481\n",
      "  Val Loss: 1.2915, Val Acc: 0.6550\n",
      "Epoch [31/50]:\n",
      "  Train Loss: 0.1578, Train Acc: 0.9516\n",
      "  Val Loss: 1.3058, Val Acc: 0.6614\n",
      "  New best validation accuracy: 0.6614\n",
      "Epoch [31/50]:\n",
      "  Train Loss: 0.1578, Train Acc: 0.9516\n",
      "  Val Loss: 1.3058, Val Acc: 0.6614\n",
      "  New best validation accuracy: 0.6614\n",
      "Epoch [32/50]:\n",
      "  Train Loss: 0.1448, Train Acc: 0.9574\n",
      "  Val Loss: 1.3556, Val Acc: 0.6606\n",
      "Epoch [32/50]:\n",
      "  Train Loss: 0.1448, Train Acc: 0.9574\n",
      "  Val Loss: 1.3556, Val Acc: 0.6606\n",
      "Epoch [33/50]:\n",
      "  Train Loss: 0.1388, Train Acc: 0.9583\n",
      "  Val Loss: 1.2880, Val Acc: 0.6622\n",
      "  New best validation accuracy: 0.6622\n",
      "Epoch [33/50]:\n",
      "  Train Loss: 0.1388, Train Acc: 0.9583\n",
      "  Val Loss: 1.2880, Val Acc: 0.6622\n",
      "  New best validation accuracy: 0.6622\n",
      "Epoch [34/50]:\n",
      "  Train Loss: 0.1270, Train Acc: 0.9628\n",
      "  Val Loss: 1.4124, Val Acc: 0.6528\n",
      "Epoch [34/50]:\n",
      "  Train Loss: 0.1270, Train Acc: 0.9628\n",
      "  Val Loss: 1.4124, Val Acc: 0.6528\n",
      "Epoch [35/50]:\n",
      "  Train Loss: 0.1461, Train Acc: 0.9523\n",
      "  Val Loss: 1.4278, Val Acc: 0.6540\n",
      "Epoch [35/50]:\n",
      "  Train Loss: 0.1461, Train Acc: 0.9523\n",
      "  Val Loss: 1.4278, Val Acc: 0.6540\n",
      "Epoch [36/50]:\n",
      "  Train Loss: 0.1156, Train Acc: 0.9652\n",
      "  Val Loss: 1.3942, Val Acc: 0.6567\n",
      "Epoch [36/50]:\n",
      "  Train Loss: 0.1156, Train Acc: 0.9652\n",
      "  Val Loss: 1.3942, Val Acc: 0.6567\n",
      "Epoch [37/50]:\n",
      "  Train Loss: 0.1083, Train Acc: 0.9690\n",
      "  Val Loss: 1.4619, Val Acc: 0.6562\n",
      "Epoch [37/50]:\n",
      "  Train Loss: 0.1083, Train Acc: 0.9690\n",
      "  Val Loss: 1.4619, Val Acc: 0.6562\n",
      "Epoch [38/50]:\n",
      "  Train Loss: 0.0813, Train Acc: 0.9796\n",
      "  Val Loss: 1.3934, Val Acc: 0.6669\n",
      "  New best validation accuracy: 0.6669\n",
      "Epoch [38/50]:\n",
      "  Train Loss: 0.0813, Train Acc: 0.9796\n",
      "  Val Loss: 1.3934, Val Acc: 0.6669\n",
      "  New best validation accuracy: 0.6669\n",
      "Epoch [39/50]:\n",
      "  Train Loss: 0.0697, Train Acc: 0.9842\n",
      "  Val Loss: 1.4220, Val Acc: 0.6626\n",
      "Epoch [39/50]:\n",
      "  Train Loss: 0.0697, Train Acc: 0.9842\n",
      "  Val Loss: 1.4220, Val Acc: 0.6626\n",
      "Epoch [40/50]:\n",
      "  Train Loss: 0.0634, Train Acc: 0.9869\n",
      "  Val Loss: 1.4239, Val Acc: 0.6632\n",
      "Epoch [40/50]:\n",
      "  Train Loss: 0.0634, Train Acc: 0.9869\n",
      "  Val Loss: 1.4239, Val Acc: 0.6632\n",
      "Epoch [41/50]:\n",
      "  Train Loss: 0.0595, Train Acc: 0.9880\n",
      "  Val Loss: 1.4312, Val Acc: 0.6653\n",
      "Epoch [41/50]:\n",
      "  Train Loss: 0.0595, Train Acc: 0.9880\n",
      "  Val Loss: 1.4312, Val Acc: 0.6653\n",
      "Epoch [42/50]:\n",
      "  Train Loss: 0.0587, Train Acc: 0.9884\n",
      "  Val Loss: 1.4757, Val Acc: 0.6610\n",
      "Epoch [42/50]:\n",
      "  Train Loss: 0.0587, Train Acc: 0.9884\n",
      "  Val Loss: 1.4757, Val Acc: 0.6610\n",
      "Epoch [43/50]:\n",
      "  Train Loss: 0.0514, Train Acc: 0.9912\n",
      "  Val Loss: 1.4407, Val Acc: 0.6659\n",
      "Early stopping at epoch 43\n",
      "Training completed! Best validation accuracy: 0.6669\n",
      "Epoch [43/50]:\n",
      "  Train Loss: 0.0514, Train Acc: 0.9912\n",
      "  Val Loss: 1.4407, Val Acc: 0.6659\n",
      "Early stopping at epoch 43\n",
      "Training completed! Best validation accuracy: 0.6669\n"
     ]
    }
   ],
   "source": [
    "# Start training model\n",
    "best_val_acc = 0\n",
    "counter = 0\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        # Convert one-hot y to class indices for CrossEntropyLoss\n",
    "        y_indices = torch.argmax(y, dim=1) # [0,1,0,0] -> 1\n",
    "        \n",
    "        loss = criterion(output, y_indices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        train_correct += (pred == y_indices).sum().item()\n",
    "        train_total += y.size(0)\n",
    "    \n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_acc_list.append(train_acc)\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            output = model(x)\n",
    "            \n",
    "            # Convert one-hot y to class indices for loss calculation\n",
    "            y_indices = torch.argmax(y, dim=1)\n",
    "            \n",
    "            loss = criterion(output, y_indices)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            val_correct += (pred == y_indices).sum().item()\n",
    "            val_total += y.size(0)\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_acc_list.append(val_acc)\n",
    "    # Print epoch results\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}]:')\n",
    "    print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), '../model/best_model.pth')\n",
    "        print(f'  New best validation accuracy: {best_val_acc:.4f}')\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "    if counter >= PATIENCE:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "        \n",
    "print(f'Training completed! Best validation accuracy: {best_val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a8c4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.434282020213064\n",
      "Test accuracy: 0.6510480887792849\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        output = model(x)\n",
    "        \n",
    "        # Convert one-hot y to class indices for loss calculation\n",
    "        y_indices = torch.argmax(y, dim=1)\n",
    "        \n",
    "        loss = criterion(output, y_indices)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Calculate validation accuracy\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        test_correct += (pred == y_indices).sum().item()\n",
    "        test_total += y.size(0)\n",
    "\n",
    "# Calculate average validation loss and accuracy\n",
    "avg_test_loss = test_loss / len(val_loader)\n",
    "test_acc = test_correct / test_total\n",
    "print(f'Average test loss: {avg_test_loss}')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff14e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
